# 自然言語処理勉強会
### 20220522

## 1. 勉強会の目標
- トークン化
- Attention
- Trasformer
- BERT
- 基本的な処理
  - 固有表現抽出
  - 類似度比較
  - 文章分類（ネガポジ判定など）
  - 文章生成
  - 文章構成
  - 文章構造化
- 最近の派生系
  - XLNet
  - ALBERT

## 2. 教材
- [BERTによる自然言語処理入門: Transformersを使った実践プログラミング | ストックマーク株式会社, 近江 崇宏, 金田 健太郎, 森長 誠, 江間見 亜利 |本 | 通販 | Amazon](https://www.amazon.co.jp/dp/427422726X/)
- 他随時

## 3. 自然言語処理概観
### 機械学習による自然言語処理
機械学習で何らかの問題を解く際には、大量のデータをもとに有用な特徴量を自動的に抽出する。
ここで、処理の流れは以下のようになる。

1. 文章からタスクを解くのに有用な特徴量を抽出する
2. 抽出した特徴量をモデルに入力し、問題を解く

BERTを含むニューラル言語モデルでは、文章や単語を密なベクトル（**分散表現**）に変換し、特徴量として用いる。

BERTは、Attentionを利用することで、前後の文脈を考慮した文章の分散表現を学習でき、さまざまなNLPタスクに転移学習が可能。
具体的には、以下の特徴を持つ。
- Transformer Encoderをアーキテクチャに持つ
  - Multi Head Attention + Feedforwardネットワーク
- 事前学習にMasked Language Modeling(MLM) + Next Sentense Prediction(NSP)を使用



## 4. トークン化


## 5. 言語モデル

## 6. Word2Vec

## 6. ELMo

