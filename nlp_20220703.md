# 自然言語処理勉強会
### 20220703

## 今日の話題
### Amazon CodeWhisperer
https://aws.amazon.com/jp/blogs/aws/now-in-preview-amazon-codewhisperer-ml-powered-coding-companion/

MLによるコード生成。GitHub Copilotみたいな。


# 3. BERT
原著：  
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ( https://arxiv.org/abs/1810.04805 )  
Attention is All You Need ( https://arxiv.org/abs/1706.03762 )
## 3-1. BERTの構造
<img src="https://camo.qiitausercontent.com/0e5daa94ef94f96e416f7dc76923726e0749dcde/68747470733a2f2f696d6775722e636f6d2f4f70676d734f342e706e67" width="60%">

RNN、CNNの代わりに、Self-Attentionを使用し各単語の関連度スコアを計算する（行列計算）  

### 3-1-a. Multi Head Attention (再掲)
Attentionとは、文中のある単語の意味を理解する時に、文中の単語のどれに注目すれば良いかを表すスコア。  
その文に出現する単語だけで内積をとり、単語間の関連度スコアを計算する（Self-Attention）  
→ **線形な処理**

具体的には、QueryとKeyでAttentionスコアを計算し、そのAttentionスコアを使ってValueを加重和すると、Attentionを適用した単語の潜在表現が手に入る  
Query, Key, Valueは、入力単語$x_i$にそれぞれ重み$W^Q,W^K,W^V$を掛け合わせたもの  (Scaled Dot-Product Attention)
$q_i=x_iW^Q$  
$k_i=x_iW^K$  
$v_i=x_iW^V$  

このQuery, Key, Valueを複数パターン用意してそれぞれScaled Dot-Product Attentionを適用し、最後に出力を一つに集約する = Multi-Head Attention。

### 3-1-b. Add + Layer Normalization
- Residual Connection : 深い層を持つ場合でも学習が進むように
- Layer Normalization : 学習の高速化のために正規化

### 3-1-c. Feedforward Network
Transformer Encoderの出力に、2層のNeural Netを挟む。GELU関数を噛ませることで**非線形性**を担保。

## 3-2. 入力形式
文章を2つ入力。トークン化した後に、トークン列の先頭に`[CLS]`、文間と末尾に`[SEP]`を加える。

```
'[CLS]', '今日', 'の', '天気', 'は', '雪', 'だっ', 'た', '。','[SEP]', '明日', 'の', '天気', 'は', '?', '[SEP]'
```

[CLS]に対応する出力はその文章自体の分散表現として用い、分類問題などで活用する。(CLS = Classifier)

トークン化の後、「Input Embedding」モジュールで各トークンをベクトル化
$e_i=e_i^T + e_i^S + e_i^P$ 
$e_i^T$ だけを入力にすると文章内の順番が考慮されない -> **入力自体に情報を埋め込んで解決。**
- トークン自体を表すToken Embedding
- 文章の順番を表すSegment Embedding
- トークンの位置を表すPosition Embedding

## 3-3. 学習
大量のデータで「事前学習」したモデルに対して、比較的少ないデータで個別タスクに特化させる「ファインチューニング」させるのが一般的。

### 3-3-a. 事前学習
以下の二つのタスクに取り組む。これらのタスクは教師なし学習なのでラベリング不要であり、文章のデータさえ集めれば実行できるのが最大のメリット。

- Masked Language Model (MLM)
    - 入力の15%の単語をマスクし、その単語を予測する。
    - 一つの文章内の文法や単語の意味の理解に役立ってるというイメージ
- Next Sentence Prediction (NSP、次文予測)
    - データの50%を連続する2文、残り50%をランダムに繋げた2文として、入力された文章が連続したものかそうでないかを予測する。
    - 文脈とか文章の意味の理解に役立ってるというイメージ

### 3-3-b. ファインチューニング
事前学習済みのモデルに、解きたいタスクに応じた新しい層を接続して学習。
一部を事前学習時の値に固定しておくよりは、モデル全体をファインチューニングしたほうが性能は良くなるらしい。
4章以降では個別タスクへのBERTの適用方法についてコードレベルで学んでいく


# 4 Huggingface Transformers
https://colab.research.google.com/github/stockmarkteam/bert-book/blob/master/Chapter4.ipynb

### 使用フレームワーク
[Huggingface](https://huggingface.co/): 米国のHugging Face社が提供している自然言語処理に特化したOSSフレームワーク。特にTransformer関連が強い。

Fugashi: MeCabをPythonから使えるようにしたライブラリ
ipadic: Mecabで使用する辞書

### Tokenize
「Mecabで単語分割」した後、「WordPieceでサブワード分割」
WordPiece = 単語単位より細かい単位で分割するサブワード分割の手法の一つ。Googleさんが実装。低頻度な単語も短いトークンの組み合わせによって表現できるじゃんという考え方。(例: 「マシンラーニング」-> 「マシン」「##ラー」「##ニング」)


# 5 文章の穴埋め

https://colab.research.google.com/github/stockmarkteam/bert-book/blob/master/Chapter5.ipynb

Masked Language Modelのタスクを解く (ファインチューニングなし)


# 6 文章分類
https://colab.research.google.com/github/stockmarkteam/bert-book/blob/master/Chapter6.ipynb

文章分類を解きます
