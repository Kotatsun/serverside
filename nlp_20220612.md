# 自然言語処理勉強会
### 20220612

## 注目トピック
### Imagen
https://imagen.research.google/

Google Research Brain Teamが開発した超高精細なText-to-Imageモデル。
テキストエンコーダーの巨大化、画素の値域を[-1, 1]に限定しないなどの工夫により、これまでの生成系のものに比べて高精細かつ写実的な画像を生成できるようになった。


## 2-2 言語モデル
文章の出現のしやすさをモデル化（数式化）したいが、どう表現するか？  
→ ある文脈のもとで、どういった単語が自然なのかを確率で表現する。  

例） 私は今日の昼食に蕎麦を（　）  
a. 食べた  
b. 歩いた  

文章としてaの方が自然なので、文脈c「私は今日の昼食に蕎麦を」のもとで、単語wi「食べた」が現れる確率が高くなるようにモデルを学習させれば良い。


## 2-3 Word2Vec
Word2Vecは次のような特徴を持った言語モデルの一種である。
- 単語埋め込み（Word Embedding）：  
分割された各トークン（単語）に対して、文脈に依存しない一意の分散表現を与えるようにモデルを学習させる。
- 加法構成性：  
参考：https://qiita.com/suzuki_sh/items/850b282cad5f189e7c4d#:~:text=%E5%95%8F%E9%A1%8C%E3%81%AA%E3%81%AE%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82-,%E5%88%86%E5%B8%83%E4%BB%AE%E8%AA%AC%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E6%84%8F%E5%91%B3%E8%AB%96,-%E5%8D%98%E8%AA%9E%E3%82%92%E7%B5%B1%E8%A8%88

Word2VecのPython実装は、[gensim](https://gotutiyan.hatenablog.com/entry/2021/01/14/011030)などで提供されており、 以下の２つのモデルが提案されている。
### 1. CBOW(Continuous Bag of Words)モデル  
<img src="https://s3-ap-northeast-1.amazonaws.com/ledge-ai-assets/media/wp-content/uploads/2020/03/05104301/image6.jpg" width="50%"> 
周辺単語(文脈c)から虫食い単語$w_i$を、予測するモデル。  
以下の２つの層で構成される。

・文脈$\textbf{c}$の単語をベクトルに変換する層 Vc → 各単語をベクトル化、平均 → 文脈ベクトル $\textbf{\it v}_c(\textbf{\it c})$  
・予測する単語をベクトルに変換する層 Vt → 予測単語$w_i$をベクトル化 → 予測単語ベクトル $\textbf{\it v}_t(w_i)$  
 
文脈$\textbf{c}$のもとで、単語$w_t$が出現する確率は、  

$p(w_i|\textbf{c})=\textbf{\it v}_c(\textbf{\it c})\cdot\textbf{\it v}_t(w_i)$　  **(内積)**  

→ 文脈と単語のベクトルが同じ方向を向いている、つまり類似している場合にその単語が出現しやすい。（**分布仮説**）


### 2. Skip-gramモデル 
<img src="https://s3-ap-northeast-1.amazonaws.com/ledge-ai-assets/media/wp-content/uploads/2020/03/05104303/image8.jpg" width="50%"> 

ある単語$w_i$に対して、その周辺単語$w$を予測するモデル。  
CBOWと因果関係が逆。  
参考：https://ledge.ai/word2vec/#:~:text=%E5%B7%A6%E5%9B%B3%E3%81%8CCBOW%E3%80%81%E5%8F%B3%E5%9B%B3%E3%81%8CSkip%2Dgram%E3%81%AE%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%88NN%EF%BC%89%E3%81%AE%E6%A7%8B%E9%80%A0%E3%80%82%E5%85%A5%E5%8A%9B%E5%B1%A4%EF%BC%88Input%20layer%EF%BC%89%E3%80%81%E9%9A%A0%E3%82%8C%E5%B1%A4%EF%BC%88Hidden%20layer%EF%BC%89%E3%80%81%E5%87%BA%E5%8A%9B%E5%B1%A4%EF%BC%88Output%20layer%EF%BC%89%E3%81%AE2%E5%B1%A4NN%E6%A7%8B%E9%80%A0%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%82%8F%E3%81%8B%E3%82%8B%E3%80%82


## 2-4. ELMo
<img src="https://data-analytics.fun/wp-content/uploads/2021/04/image-3.png" width="60%">

以下のような特徴を持つ  
- **文脈化単語埋め込み**  ：ELMoで得られる分散表現は、文脈に応じた値を取る。  
- 双方向LSTM(Long Short-Term Memory)  ：RNNの一種  
- 層間にResidual Connectionを持つ  

### RNN（再帰型ニューラルネット）  
時刻iの出力がそれまでの全ての入力(1 ~ i-1)を考慮して計算される(順方向RNN)  
⇄ 時刻iの出力がそれ以降全ての入力(i+1 ~ n)を考慮して計算される(逆方向RNN)  

入力された情報を長時間保持できない（線形に情報を加算していくので「薄められる」）  
→ 長い文脈を考慮できない

### LSTM(Long Short-Term Memory)    
RNNの一種。  
入力xiの値に応じて、次の値をを更新する。 

①入力の情報の重み gI  
②前時刻の情報の重み gF  
③次時刻に伝搬させる情報の重み  

→ 入力された情報を長時間保持できる  


ELMoの事前学習
文脈中のi番目の単語wiを予測するような学習を行う    
・順方向多層LSTM→pt(wi | w1, w2, … , wi-1)  
・逆方向多層LSTM→pr(wi | wi+1, wi+2, … , wn)  
→ pt × prのlogを取り、損失関数とする  

*RNNは時刻tの計算が終わるまで時刻t+1の計算を始められない。  
→ **並列化できない！めちゃくちゃ学習に時間がかかる！**

<img src="https://deepage.net/img/rnn/rnn-top.jpg" width="50%">


# 3. BERT
原著：  
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ( https://arxiv.org/abs/1810.04805 )  
Attention is All You Need ( https://arxiv.org/abs/1706.03762 )
## 3-1. BERTの構造
<img src="https://camo.qiitausercontent.com/0e5daa94ef94f96e416f7dc76923726e0749dcde/68747470733a2f2f696d6775722e636f6d2f4f70676d734f342e706e67" width="60%">

RNN、CNNの代わりに、Self-Attentionを使用し各単語の関連度スコアを計算する（行列計算）  
→ **並列化可能！学習が高速に行える！**

### 1. Multi Head Attention
Attentionとは、文中のある単語の意味を理解する時に、文中の単語のどれに注目すれば良いかを表すスコア。  
その文に出現する単語だけで内積をとり、単語間の関連度スコアを計算する（Self-Attention）  
→ **線形な処理**

具体的には、QueryとKeyでAttentionスコアを計算し、そのAttentionスコアを使ってValueを加重和すると、Attentionを適用した単語の潜在表現が手に入る  
Query, Key, Valueは、入力単語$x_i$にそれぞれ重み$W^Q,W^K,W^V$を掛け合わせたもの  
$q_i=x_iW^Q$  
$k_i=x_iW^K$  
$v_i=x_iW^V$  

### 2. Feedforward Network
Transformer Encoderの出力に、GELU関数を噛ませることで**非線形性**を担保することができる




