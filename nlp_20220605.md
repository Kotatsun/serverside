# 自然言語処理勉強会
### 20220522

## 注目トピック
### Imagen
https://imagen.research.google/

Google Research Brain Teamが開発した超高精細なText-to-Imageモデル。
テキストエンコーダーの巨大化、画素の値域を[-1,1]に限定しないなどの工夫により、これまでの生成系のものに比べて高精細かつ写実的な画像を生成できるようになった。


## 2-2 言語モデル
文章の出現のしやすさをモデル化（数式化）したいが、どう表現するか？  
→ ある文脈のもとで、どういった単語が自然なのかを確率で表現する。  

例） 私は今日の昼食に蕎麦を（　）  
a. 食べた  
b. 歩いた  

文章としてaの方が自然なので、文脈c「私は今日の昼食に蕎麦を」のもとで、単語wi「食べた」が現れる確率が高くなるようにモデルを学習させれば良い。


## 2-3 Word2Vec
Word2Vecは次のような特徴を持った言語モデルの一種である。
- 単語埋め込み（Word Embedding）  
分割された各トークン（単語）に対して、文脈に依存しない一意の分散表現を与えるようにモデルを学習させる。
- 加法構成性  
参考：https://qiita.com/suzuki_sh/items/850b282cad5f189e7c4d#:~:text=%E5%95%8F%E9%A1%8C%E3%81%AA%E3%81%AE%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82-,%E5%88%86%E5%B8%83%E4%BB%AE%E8%AA%AC%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E6%84%8F%E5%91%B3%E8%AB%96,-%E5%8D%98%E8%AA%9E%E3%82%92%E7%B5%B1%E8%A8%88

Word2VecのPython実装は、[gensim](https://gotutiyan.hatenablog.com/entry/2021/01/14/011030)などで提供されており、 以下の２つのモデルが提案されている。
### CBOW(Continuous Bag of Words)モデル  
虫食い単語wtを、ウィンドウサイズcの周辺単語から予測するモデル。以下の２つの層で構成される。

・文脈cの単語をベクトルに変換する層 Vc → 各単語をベクトル化、平均 → **v**(c)  
・予測する単語をベクトルに変換する層 Vt → 予測単語をベクトル化 → **v**(wt)  
 
単語wtの出現確率は、  

p=**v**(c)・**v**(wt)　**内積**  

→ 文脈と単語のベクトルが同じ方向を向いている、つまり類似している場合にその単語が出現しやすい。（**分布仮説**）


### skip-gramモデル  


## 2-4 ELMo

